# Performance Evaluation Exercise

We now train a plane detector on satellite images.

**Example:**

![alt text](https://storage.googleapis.com/dp-missions/hiring-sa/plane.png "Single axle trees")

You have access to:
- all the images in the `images` directory
- the ground truth boxes, checked by human annotators, in `groundtruth.json`
- the predicted boxes, generated by the detector, in `predictions.json`

### Ground truth

The ground truth boxes are in the following format:
```
{
  "images": [
    {
      "annotated_regions": [
        {
          "tags": [
            "avion"
          ],
          "region_type": "Box",
          "region": {
            "xmin": 0.5411571108172154,
            "xmax": 0.6228667914646092,
            "ymin": 0.32151435917628496,
            "ymax": 0.4340908938779274
          }
        }
      ],
      "location": "planes/pic6.JPG"
    }, ...
}
```


### Predictions

Each predicted box is in the following format:
```
{
  "images": [
    {
      "annotated_regions": [
        {
          "tags": [
            "avion"
          ],
          "score": 0.832
          "region_type": "Box",
          "region": {
            "xmin": 0.5411571108172154,
            "xmax": 0.6228667914646092,
            "ymin": 0.32151435917628496,
            "ymax": 0.4340908938779274
          }
        }
      ],
      "location": "planes/pic6.JPG"
    }, ...
}
```

Notice that there is now a confidence score between 0 and 1.

## Task 1

Your first task is to implement a function that takes an image, then compares the ground truth to the
predictions above a given score threshold, and returns:
- the number of true positives
- the number of false positives
- the number of false negatives

The function is called `evaluate_image` and you just need to write its body. Here are the definition of these metrics.

### definition: IoU

The **IoU** (intersection over union) `IoU(B1, B2)` of two boxes is defined as the surface of the intersection of the two boxes divided by the surface of the union of the two boxes.

### definition: True positive

A predicted box P is a **true positive** if there is a ground truth box G such that IoU(P,G) >= 0.5, such that G is not already associated with another predicted box.

If two predicted boxes satisfy the IoU criterion, only the one with the highest score will be associated with the ground truth box.

### definition: False negative

A false negative is a ground truth box that is not associated with any predicted box.

### definition: False positive

A predicted box P is a **false positive** if it is not associated with any ground truth box.


## Task 2

Your second task is to compute the Precision and Recall metrics. The definition of precision and recall can be found on [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall). It is defined on the __whole dataset__ (and not at the image level).

**Remark**: the precision and recall are defined for a certain score treshold (cf. Task 1).

Your task is to compute these two metrics for all thresholds in the `[0.0; 1.0]` range, with a certain granularity. The function is called `evaluate_pr_naive` and you just need to write its body. It takes as arguments the whole groundtruth (as defined in `groundtruth.json`) and the whole set of predictions (as defined in `predictions.json`), and compute the prediction and recall on the __whole dataset__.

Example of function call:
```
evaluate_pr(groundtruth, predictions, N=10, Jaccard_min=0.5)
```

This will return the precision / recall for the list of thresholds `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]`, in the following form:

```
[{"precision": 0.2, "recall": 0.9, "threshold": 0.1}, {"precision": 0.3, "recall": 0.8, "threshold": 0.2}, ...]
```

For this task, you will be judged on the correctness of the functions, but also the elegance of your code.

## Task 3 

Finally we would like you to submit an optimized implementation of your function: the goal is not to use faster library but rather to work on your solution to reduce the amount of unnecessary computation. Your function should be coded in the method `evaluate_pr`. We use the `time` library to compare the efficiency between each implementation. 